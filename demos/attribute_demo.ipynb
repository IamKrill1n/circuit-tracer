{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa5r1-7RmS8j"
   },
   "source": [
    "# Attribution Demo \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/safety-research/circuit-tracer/blob/main/demos/attribute_demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In this demo, you'll learn how to load models and perform attribution on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Setup Environment\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    !mkdir -p repository && cd repository && \\\n",
    "     git clone https://github.com/safety-research/circuit-tracer && \\\n",
    "     curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
    "     uv pip install -e circuit-tracer/\n",
    "\n",
    "    import sys\n",
    "    from huggingface_hub import notebook_login\n",
    "    sys.path.append('repository/circuit-tracer')\n",
    "    sys.path.append('repository/circuit-tracer/demos')\n",
    "    notebook_login(new_session=False)\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # import sys\n",
    "    # sys.path.insert(0, \"/home/tu/circuit-tracer/circuit_tracer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "P8fNhpqzmS8k"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils.create_graph_files import create_graph_files_topk\n",
    "from circuit_tracer.graph import Graph, prune_graph, prune_graph_topk, compute_graph_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN_3kEyfmS8k"
   },
   "source": [
    "First, load your model and transcoders by name. `model_name` is a normal HuggingFace / [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) model name; we'll use `google/gemma-2-2b`. We set `transcoder_name` to `gemma`, which is shorthand for the [Gemma Scope](https://arxiv.org/abs/2408.05147) transcoders; we take the transcoders with lowest L0 (mean # of active features) for each layer.\n",
    "\n",
    "We additionally support `model_name = \"meta-llama/Llama-3.2-1B\"`, with `\"llama\"` transcoders; these are ReLU skip-transcoders that we trained, available [here](https://huggingface.co/mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/tree/new-training).\n",
    "\n",
    "If you want to use other models, you'll have to provide your own transcoders. To do this, set `transcoder_name` to point to your own configuration file, specifying the list of transcoders that you want to use. You can see `circuit_tracer/configs` for example configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBsETpl0mS8l"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2384eff4ac0544a6b1526419611de89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90edd918682c4f83a7a22210a516004e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 52 files:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056a9fe5f31b48e9806540ee1d8c2a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_10.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714f602238294d8ba996ef4745471339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_11.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ec4bebea9c4e34bf8039384fc294d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_14.safetensors:   0%|          | 0.00/906M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6caa4f25004694864f33602657b274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_13.safetensors:   0%|          | 0.00/981M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abfc01f5f48480a8d8f138c56bcfb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_1.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bbf6f98b7a47fe95f052acafc03ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_15.safetensors:   0%|          | 0.00/830M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b12579eb24fcca97e36f471469c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_12.safetensors:   0%|          | 0.00/1.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967740e881334686914ce25096f57af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_0.safetensors:   0%|          | 0.00/1.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ea6b420441430a94192f1d530e8fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_17.safetensors:   0%|          | 0.00/679M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7c9303d3194667b6b594caeb9ff54e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_2.safetensors:   0%|          | 0.00/1.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97d950e3a654506b07b0c9b9bb88e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_19.safetensors:   0%|          | 0.00/528M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ec405cae984ca3992fd5279813c7b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_20.safetensors:   0%|          | 0.00/453M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de44b7ced9624d31b1da195928a1dee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_21.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db5bd6931c74cc6b82e8db651796ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_16.safetensors:   0%|          | 0.00/755M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88e1cc2ce5e42439a3e48e4ba7ade40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "W_dec_18.safetensors:   0%|          | 0.00/604M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"genma\" #\"gemma\" mntss/clt-gemma-2-2b-426k\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcZNR0egmS8l"
   },
   "source": [
    "Next, set your attribution arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5XBwNyq4mS8l"
   },
   "outputs": [],
   "source": [
    "prompt = \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload='disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXfD-5GrmS8l"
   },
   "source": [
    "Then, just run attribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wx2XiXVjmS8l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.49s\n",
      "Found 6371 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.09s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7188\n",
      "Will include 6371 of 6371 feature nodes\n",
      "Input vectors built in 1.79s\n",
      "Phase 3: Computing logit attributions\n",
      "<sys>:0: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.08s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 6371/6371 [00:00<00:00, 10741.66it/s]\n",
      "Feature attributions completed in 0.60s\n",
      "Attribution completed in 8.24s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUn1YKnUmS8l"
   },
   "source": [
    "We now have a graph object! We can save it as a .pt file, but be warned that it's large (~167MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2tLE4FzdmS8m"
   },
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "# graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prune_graph_topk' from 'circuit_tracer.graph' (/home/tu/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/graph.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcircuit_tracer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Graph, prune_graph, prune_graph_topk, compute_graph_scores\n\u001b[32m      2\u001b[39m graph = Graph.from_pt(graph_path)\n\u001b[32m      3\u001b[39m node_mask, edge_mask, graph_score = topk_prune_graph(graph, top_k = \u001b[32m3\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'prune_graph_topk' from 'circuit_tracer.graph' (/home/tu/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/graph.py)"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.graph import Graph, prune_graph, prune_graph_topk, compute_graph_scores\n",
    "graph = Graph.from_pt(graph_path)\n",
    "node_mask, edge_mask, graph_score = topk_prune_graph(graph, top_k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(33), tensor(31))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_mask.sum(), node_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 288: Feature\n",
      "  Active feature: tensor([   0,    4, 7750])\n",
      "Node 505: Feature\n",
      "  Active feature: tensor([   0,    6, 5626])\n",
      "Node 854: Feature\n",
      "  Active feature: tensor([   1,    6, 4767])\n",
      "Node 1161: Feature\n",
      "  Active feature: tensor([   2,    6, 9457])\n",
      "Node 1427: Feature\n",
      "  Active feature: tensor([   3,    6, 5892])\n",
      "Node 1843: Feature\n",
      "  Active feature: tensor([    4,     6, 13154])\n",
      "Node 3758: Feature\n",
      "  Active feature: tensor([   7,    6, 6861])\n",
      "Node 5310: Feature\n",
      "  Active feature: tensor([  14,    6, 2268])\n",
      "Node 5414: Feature\n",
      "  Active feature: tensor([16,  6, 25])\n",
      "Node 5717: Feature\n",
      "  Active feature: tensor([   20,     7, 15589])\n",
      "Node 6393: Error\n",
      "Node 6425: Error\n",
      "Node 6570: Error\n",
      "Node 6579: Token\n",
      "Node 6580: Token\n",
      "Node 6581: Token\n",
      "Node 6582: Token\n",
      "Node 6583: Token\n",
      "Node 6584: Token\n",
      "Node 6585: Token\n",
      "Node 6586: Token\n",
      "Node 6587: Logit\n",
      "Node 6588: Logit\n",
      "Node 6589: Logit\n",
      "Node 6590: Logit\n",
      "Node 6591: Logit\n",
      "Node 6592: Logit\n",
      "Node 6593: Logit\n",
      "Node 6594: Logit\n",
      "Node 6595: Logit\n",
      "Node 6596: Logit\n"
     ]
    }
   ],
   "source": [
    "# Print out all nodes, and if a node is a Feature, print its active feature details.\n",
    "\n",
    "n_features = len(graph.active_features)         # first set of nodes: feature nodes\n",
    "n_token = len(graph.input_tokens)                # token nodes count\n",
    "n_error = graph.cfg.n_layers * n_token            # error nodes count\n",
    "\n",
    "node_attrs = {}\n",
    "for orig_idx in range(graph.adjacency_matrix.size(0)):\n",
    "    if orig_idx < n_features:\n",
    "        node_type = \"Feature\"\n",
    "        data = tuple(graph.active_features[orig_idx].tolist())\n",
    "    elif orig_idx < n_features + n_error:\n",
    "        node_type = \"Error\"\n",
    "        data = None\n",
    "    elif orig_idx < n_features + n_error + n_token:\n",
    "        node_type = \"Token\"\n",
    "        data = None\n",
    "    else:\n",
    "        node_type = \"Logit\"\n",
    "        data = None\n",
    "    node_attrs[orig_idx] = {\"type\": node_type, \"data\": data}\n",
    "\n",
    "for node in torch.where(node_mask)[0].tolist():\n",
    "    node_type = node_attrs[node][\"type\"]\n",
    "    print(f\"Node {node}: {node_type}\")\n",
    "    if node_type == \"Feature\":\n",
    "        print(\"  Active feature:\", graph.active_features[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3cdLLfJmS8m"
   },
   "source": [
    "Given this object, we can create the graph files that we need to visualize the graph. Give it a slug (name), and set the node / edge thresholds for pruning. Pruning removes unimportant nodes and edges from your graph; lower thresholds (i.e., more aggressive pruning) results in smaller graphs. These may be easier to interpret, but explain less of the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Vh8HPtimmS8m"
   },
   "outputs": [],
   "source": [
    "slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "topk = 3\n",
    "# create_graph_files(\n",
    "#     graph_or_path=graph_path,  # the graph to create files for\n",
    "#     slug=slug,\n",
    "#     output_path=graph_file_dir,\n",
    "#     node_threshold=node_threshold,\n",
    "#     edge_threshold=edge_threshold\n",
    "# )\n",
    "\n",
    "create_graph_files_topk(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    top_k = topk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQuFE-eimS8m"
   },
   "source": [
    "Now, you can visualize the graph using the following commands! This will spin up a local server to act as the frontend.\n",
    "\n",
    "**If you're running this notebook on a remote server, make sure that you set up port forwarding, so that the chosen port is accessible on your local machine too.**\n",
    "\n",
    "You can select nodes by clicking on them. Ctrl/Cmd+Click on nodes to pin and unpin them to your subgraph. G+Click on nodes in the subgraph to group them together into a supernode; G+Click on the X next to a supernode to dissolve it. Click on the edit button to edit node descriptions, and click on supernode description to edit that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gMZ8Ee-KmS8m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the IFrame below, or open your graph here: f'http://localhost:8046/index.html'\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "\n",
    "port = 8046\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import output as colab_output  # noqa\n",
    "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
    "else:\n",
    "    from IPython.display import IFrame\n",
    "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
    "    # display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGiO8jBmS8m"
   },
   "source": [
    "Once you're done, you can stop the server with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "185O1Ck1mS8m"
   },
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98579UbGmS8m"
   },
   "source": [
    "Congrats, you're done! Go to `intervention_demo.ipynb` to see how to perform interventions, or check out `gemma_demo.ipynb` and `llama_demo.ipynb` for examples of worked-out test examples. Read on for a bit more info aabout the Graph class and pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkgM1cBCmS8m"
   },
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGnU9l1zmS8m"
   },
   "source": [
    "Earlier, you created a graph object. Its adjacency matrix / edge weights are stored in `graph.adjacency_matrix` in a dense format; rows are target nodes and columns are source nodes. The first `len(graph.real_features)` entries of the matrix represent features; the `i`th entry corresponds to the `i`th feature in `graph.real_features`, given in `(layer, position, feature_idx)` format. The next `graph.cfg.n_layers * graph.n_pos` entries are error_nodes. The next `graph.n_pos` entries are token nodes. The final `len(graph.logit_tokens)` entries are logit nodes.\n",
    "\n",
    "The value of the cell `graph.adjacency_matrix[target, source]` is the direct effect of the source node on the target node. That is, it tells you how much the target node's value would change if the source node were set to 0, while holding the attention patterns, layernorm denominators, and other feature activations constatnt. Thus, if the target node is a feature, this tells you how much the target feature would change; if the target node is a logit, this tells you how much the (de-meaned) value of the logit would change.\n",
    "\n",
    "Note that `gemma-2-2b` is model (family) that uses logit softcapping. This means that a softcap function, `softcap(x) = t * tanh(x/t)` is used to constrain the logits to fall within (-t, t); `gemma-2-2b` uses `t=30`. For such models, we predict the change in logits *pre-softcap*, as the nonlinearity introduced by softcapping would cause our attribution to yield incorrect / approximate direct effect values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWTV8i9zmS8n"
   },
   "source": [
    "### Pruning\n",
    "Given a graph, you might want to prune it, as it will otherwise contain many low-impact nodes and edges that clutter the circuit diagram while adding little information. We enable you to prune nodes by absolute influence, i.e. the total impact that the nodes have on the logits, direct and indirect. The default threshold is 0.8: this means we will keep the minimum number of nodes required to capture 80% of all logit effects. Similarly, the edge_threshold, by default 0.98, means that we will keep the minimum number of edges required to capture 98% of all logit effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GmKhWpuUmS8n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6597"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from circuit_tracer.graph import Graph, prune_graph, topk_prune_graph, compute_graph_scores\n",
    "\n",
    "# Load the graph (change the path as needed)\n",
    "graph = Graph.from_pt(\"graphs/example_graph.pt\")\n",
    "n_tokens = len(graph.input_tokens)\n",
    "n_logits = len(graph.logit_tokens)\n",
    "# Prune the graph (adjust thresholds as desired)\n",
    "prune_result = prune_graph(graph, node_threshold=0.8, edge_threshold=0.98)\n",
    "\n",
    "# Access the pruned properties\n",
    "node_mask = prune_result.node_mask\n",
    "edge_mask = prune_result.edge_mask\n",
    "\n",
    "# Optionally, extract the pruned adjacency matrix:\n",
    "# pruned_adjacency = graph.adjacency_matrix[node_mask][:, node_mask]\n",
    "graph.adjacency_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7157570123672485, 0.9245588183403015)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_score, completeness_score = compute_graph_scores(graph)\n",
    "replacement_score, completeness_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6371, 6371)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(graph.selected_features), len(graph.active_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgraph nodes (original indices): [288, 505, 854, 1161, 1427, 1843, 3758, 5310, 5414, 5717, 6393, 6425, 6570, 6579, 6581, 6582, 6583, 6585, 6587]\n",
      "Subgraph adjacency matrix shape: torch.Size([19, 19])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def topk_prune_graph(graph, node_mask, edge_mask, top_k=3):\n",
    "    n_tokens = len(graph.input_tokens)\n",
    "    n_logits = len(graph.logit_tokens)\n",
    "    total_nodes = graph.adjacency_matrix.size(0)\n",
    "    \n",
    "    # Identify highest logit node (nodes at the end of the graph)\n",
    "    highest_logit_rel = torch.argmax(graph.logit_probabilities).item()\n",
    "    highest_logit_node = total_nodes - n_logits + highest_logit_rel\n",
    "\n",
    "    visited = set()\n",
    "    edge_list = []  # will hold tuples of (src, tgt, weight)\n",
    "\n",
    "    def dfs(node_idx):\n",
    "        if node_idx in visited:\n",
    "            return\n",
    "        if not node_mask[node_idx]:\n",
    "            return\n",
    "        visited.add(node_idx)\n",
    "        \n",
    "        # Get the row corresponding to incoming effects for this target node.\n",
    "        row = graph.adjacency_matrix[node_idx]\n",
    "        valid_edges = edge_mask[node_idx] & node_mask\n",
    "        filtered_row = row.clone()\n",
    "        filtered_row[~valid_edges] = 0.0\n",
    "        \n",
    "        if torch.sum(filtered_row.abs()) == 0:\n",
    "            return\n",
    "        \n",
    "        nonzero_idx = (filtered_row.abs() > 0).nonzero(as_tuple=True)[0]\n",
    "        if len(nonzero_idx) == 0:\n",
    "            return\n",
    "        \n",
    "        cur_top_k = min(top_k, len(nonzero_idx))\n",
    "        # Use absolute values to choose the strongest connections.\n",
    "        top_vals, top_indices = torch.topk(filtered_row, cur_top_k)\n",
    "        for src in top_indices.tolist():\n",
    "            # Record the edge (note that row index = target, column index = source)\n",
    "            weight = graph.adjacency_matrix[node_idx, src].item()\n",
    "            edge_list.append((src, node_idx, weight))\n",
    "            dfs(src)\n",
    "\n",
    "    dfs(highest_logit_node)\n",
    "\n",
    "    # Build the subgraph: sort the visited nodes.\n",
    "    sub_nodes = sorted(visited)\n",
    "    # Map original node indices to new indices in the subgraph.\n",
    "    index_map = {orig_idx: new_idx for new_idx, orig_idx in enumerate(sub_nodes)}\n",
    "    \n",
    "    # Create a new (sparse) adjacency matrix for the subgraph.\n",
    "    new_adj = torch.zeros((len(sub_nodes), len(sub_nodes)))\n",
    "    for src, tgt, weight in edge_list:\n",
    "        if src in index_map and tgt in index_map:\n",
    "            new_src = index_map[src]\n",
    "            new_tgt = index_map[tgt]\n",
    "            # Remember: rows correspond to targets and columns to sources.\n",
    "            new_adj[new_tgt, new_src] = weight\n",
    "\n",
    "    # Determine original node types.\n",
    "    # Graph's node ordering (as per the docstring):\n",
    "    #   [Feature nodes, Error nodes, Token nodes, Logit nodes]\n",
    "    n_features = len(graph.selected_features)\n",
    "    n_token = len(graph.input_tokens)\n",
    "    n_error = graph.cfg.n_layers * n_token\n",
    "    \n",
    "    # Logit nodes are the final n_logits.\n",
    "\n",
    "    node_attrs = {}\n",
    "    for orig_idx in sub_nodes:\n",
    "        if orig_idx < n_features:\n",
    "            node_type = \"Feature\"\n",
    "            data = tuple(graph.active_features[orig_idx].tolist())\n",
    "        elif orig_idx < n_features + n_error:\n",
    "            node_type = \"Error\"\n",
    "            data = None\n",
    "        elif orig_idx < n_features + n_error + n_token:\n",
    "            node_type = \"Token\"\n",
    "            data = None\n",
    "        else:\n",
    "            node_type = \"Logit\"\n",
    "            data = None\n",
    "        node_attrs[orig_idx] = {\"type\": node_type, \"data\": data}\n",
    "    \n",
    "    # Return the subgraph information:\n",
    "    #   sub_nodes : list of original node indices in the subgraph\n",
    "    #   new_adj   : new (sparse) adjacency matrix for these nodes\n",
    "    #   node_attrs: dictionary of original node attributes (for visualization)\n",
    "    #   edge_list : list of DFS-selected edges (from original graph)\n",
    "    return sub_nodes, new_adj, node_attrs, edge_list\n",
    "\n",
    "# --- Example usage and visualization ---\n",
    "\n",
    "# Assume you've already loaded your graph object and computed:\n",
    "#  node_mask, edge_mask = prune_result.node_mask, prune_result.edge_mask\n",
    "sub_nodes, new_adj, node_attrs, edge_list = topk_prune_graph(graph, node_mask, edge_mask, top_k=3)\n",
    "print(\"Subgraph nodes (original indices):\", sub_nodes)\n",
    "print(\"Subgraph adjacency matrix shape:\", new_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature nodes: [5717, 5310, 3758, 1843, 1427, 1161, 505, 3, 1, 854, 288, 508, 479, 844, 516, 500, 496, 5414, 5582, 5497, 5328, 2639, 1529, 996, 92, 166, 986, 125, 1546, 5422, 5775, 5580, 5641, 5974, 5862, 5639]\n",
      "0: Node 5717: layer 20, position 7, feature index 15589\n",
      "1: Node 5310: layer 14, position 6, feature index 2268\n",
      "2: Node 3758: layer 7, position 6, feature index 6861\n",
      "3: Node 1843: layer 4, position 6, feature index 13154\n",
      "4: Node 1427: layer 3, position 6, feature index 5892\n",
      "5: Node 1161: layer 2, position 6, feature index 9457\n",
      "6: Node 505: layer 0, position 6, feature index 5626\n",
      "7: Node 3: layer 0, position 1, feature index 354\n",
      "8: Node 1: layer 0, position 1, feature index 96\n",
      "9: Node 854: layer 1, position 6, feature index 4767\n",
      "10: Node 288: layer 0, position 4, feature index 7750\n",
      "11: Node 508: layer 0, position 6, feature index 6116\n",
      "12: Node 479: layer 0, position 6, feature index 1847\n",
      "13: Node 844: layer 1, position 6, feature index 2132\n",
      "14: Node 516: layer 0, position 6, feature index 8163\n",
      "15: Node 500: layer 0, position 6, feature index 5215\n",
      "16: Node 496: layer 0, position 6, feature index 4687\n",
      "17: Node 5414: layer 16, position 6, feature index 25\n",
      "18: Node 5582: layer 18, position 7, feature index 8959\n",
      "19: Node 5497: layer 17, position 7, feature index 7178\n",
      "20: Node 5328: layer 15, position 2, feature index 4494\n",
      "21: Node 2639: layer 6, position 2, feature index 4662\n",
      "22: Node 1529: layer 4, position 2, feature index 7671\n",
      "23: Node 996: layer 2, position 2, feature index 11838\n",
      "24: Node 92: layer 0, position 2, feature index 1961\n",
      "25: Node 166: layer 0, position 2, feature index 9697\n",
      "26: Node 986: layer 2, position 2, feature index 8734\n",
      "27: Node 125: layer 0, position 2, feature index 5640\n",
      "28: Node 1546: layer 4, position 2, feature index 11350\n",
      "29: Node 5422: layer 16, position 7, feature index 4298\n",
      "30: Node 5775: layer 21, position 7, feature index 5943\n",
      "31: Node 5580: layer 18, position 7, feature index 6101\n",
      "32: Node 5641: layer 19, position 7, feature index 2695\n",
      "33: Node 5974: layer 23, position 7, feature index 12237\n",
      "34: Node 5862: layer 22, position 7, feature index 4999\n",
      "35: Node 5639: layer 19, position 7, feature index 1445\n"
     ]
    }
   ],
   "source": [
    "n_features = len(graph.active_features)\n",
    "\n",
    "feature_nodes = [node for node in nodes_found if node < n_features]\n",
    "print(\"Feature nodes:\", feature_nodes)\n",
    "\n",
    "# Trace back the feature data: each feature is stored as (layer, position, feature_idx)\n",
    "for i, node in enumerate(feature_nodes):\n",
    "    layer, pos, feature_idx = graph.active_features[node]\n",
    "    print(f\"{i}: Node {node}: layer {layer}, position {pos}, feature index {feature_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0728,\n",
       "         0.0136,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  3.9375,  2.1875,  1.1016,\n",
       "         0.8008,  0.6289,  0.9180, -0.0801,  2.7656,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.adjacency_matrix[5328][feature_nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
