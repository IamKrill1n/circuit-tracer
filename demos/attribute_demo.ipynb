{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa5r1-7RmS8j"
   },
   "source": [
    "# Attribution Demo \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/safety-research/circuit-tracer/blob/main/demos/attribute_demo.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In this demo, you'll learn how to load models and perform attribution on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Colab Setup Environment\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    !mkdir -p repository && cd repository && \\\n",
    "     git clone https://github.com/safety-research/circuit-tracer && \\\n",
    "     curl -LsSf https://astral.sh/uv/install.sh | sh && \\\n",
    "     uv pip install -e circuit-tracer/\n",
    "\n",
    "    import sys\n",
    "    from huggingface_hub import notebook_login\n",
    "    sys.path.append('repository/circuit-tracer')\n",
    "    sys.path.append('repository/circuit-tracer/demos')\n",
    "    notebook_login(new_session=False)\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    # import sys\n",
    "    # sys.path.insert(0, \"/home/tu/circuit-tracer/circuit_tracer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8fNhpqzmS8k"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils.create_graph_files import create_graph_files  \n",
    "from circuit_tracer.graph import Graph, prune_graph, compute_graph_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN_3kEyfmS8k"
   },
   "source": [
    "First, load your model and transcoders by name. `model_name` is a normal HuggingFace / [TransformerLens](https://github.com/TransformerLensOrg/TransformerLens) model name; we'll use `google/gemma-2-2b`. We set `transcoder_name` to `gemma`, which is shorthand for the [Gemma Scope](https://arxiv.org/abs/2408.05147) transcoders; we take the transcoders with lowest L0 (mean # of active features) for each layer.\n",
    "\n",
    "We additionally support `model_name = \"meta-llama/Llama-3.2-1B\"`, with `\"llama\"` transcoders; these are ReLU skip-transcoders that we trained, available [here](https://huggingface.co/mntss/skip-transcoder-Llama-3.2-1B-131k-nobos/tree/new-training).\n",
    "\n",
    "If you want to use other models, you'll have to provide your own transcoders. To do this, set `transcoder_name` to point to your own configuration file, specifying the list of transcoders that you want to use. You can see `circuit_tracer/configs` for example configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "login(token= str(os.getenv(\"HUGGINGFACE_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BBsETpl0mS8l"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cfd71941734acda9826cfc026e55d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 26 files:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m'\u001b[39m\u001b[33mgoogle/gemma-2-2b\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m transcoder_name = \u001b[33m\"\u001b[39m\u001b[33mgemma\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m#\"gemma\" mntss/clt-gemma-2-2b-426k\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/replacement_model.py:140\u001b[39m, in \u001b[36mReplacementModel.from_pretrained\u001b[39m\u001b[34m(cls, model_name, transcoder_set, device, dtype, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    138\u001b[39m     device = get_default_device()\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m transcoders, _ = load_transcoder_from_hub(transcoder_set, device=device, dtype=dtype)\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_pretrained_and_transcoders(\n\u001b[32m    143\u001b[39m     model_name,\n\u001b[32m    144\u001b[39m     transcoders,\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m     **kwargs,\n\u001b[32m    148\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/utils/hf_utils.py:70\u001b[39m, in \u001b[36mload_transcoder_from_hub\u001b[39m\u001b[34m(hf_ref, device, dtype, lazy_encoder, lazy_decoder)\u001b[39m\n\u001b[32m     67\u001b[39m config[\u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m] = hf_uri.revision\n\u001b[32m     68\u001b[39m config[\u001b[33m\"\u001b[39m\u001b[33mscan\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_uri.repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_uri.revision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_uri.revision \u001b[38;5;28;01melse\u001b[39;00m hf_uri.repo_id\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m load_transcoders(config, device, dtype, lazy_encoder, lazy_decoder), config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/utils/hf_utils.py:89\u001b[39m, in \u001b[36mload_transcoders\u001b[39m\u001b[34m(config, device, dtype, lazy_encoder, lazy_decoder)\u001b[39m\n\u001b[32m     86\u001b[39m     transcoder_paths = resolve_transcoder_paths(config)\n\u001b[32m     87\u001b[39m     is_gemma_scope = \u001b[33m\"\u001b[39m\u001b[33mgemma-scope\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config.get(\u001b[33m\"\u001b[39m\u001b[33mrepo_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m load_transcoder_set(\n\u001b[32m     90\u001b[39m         transcoder_paths,\n\u001b[32m     91\u001b[39m         scan=config[\u001b[33m\"\u001b[39m\u001b[33mscan\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     92\u001b[39m         feature_input_hook=config[\u001b[33m\"\u001b[39m\u001b[33mfeature_input_hook\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     93\u001b[39m         feature_output_hook=config[\u001b[33m\"\u001b[39m\u001b[33mfeature_output_hook\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     94\u001b[39m         gemma_scope=is_gemma_scope,\n\u001b[32m     95\u001b[39m         dtype=dtype,\n\u001b[32m     96\u001b[39m         device=device,\n\u001b[32m     97\u001b[39m         lazy_encoder=lazy_encoder,\n\u001b[32m     98\u001b[39m         lazy_decoder=lazy_decoder,\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m model_kind == \u001b[33m\"\u001b[39m\u001b[33mcross_layer_transcoder\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcircuit_tracer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtranscoder\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcross_layer_transcoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_clt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/transcoder/single_layer_transcoder.py:457\u001b[39m, in \u001b[36mload_transcoder_set\u001b[39m\u001b[34m(transcoder_paths, scan, feature_input_hook, feature_output_hook, device, dtype, gemma_scope, lazy_encoder, lazy_decoder)\u001b[39m\n\u001b[32m    455\u001b[39m load_fn = load_gemma_scope_transcoder \u001b[38;5;28;01mif\u001b[39;00m gemma_scope \u001b[38;5;28;01melse\u001b[39;00m load_relu_transcoder\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(transcoder_paths)):\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     transcoders[layer] = load_fn(\n\u001b[32m    458\u001b[39m         transcoder_paths[layer],\n\u001b[32m    459\u001b[39m         layer,\n\u001b[32m    460\u001b[39m         device=device,\n\u001b[32m    461\u001b[39m         dtype=dtype,\n\u001b[32m    462\u001b[39m         lazy_encoder=lazy_encoder,\n\u001b[32m    463\u001b[39m         lazy_decoder=lazy_decoder,\n\u001b[32m    464\u001b[39m     )\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# we don't know how many layers the model has, but we need all layers from 0 to max covered\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(transcoders.keys()) == \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmax\u001b[39m(transcoders.keys()) + \u001b[32m1\u001b[39m)), (\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEach layer should have a transcoder, but got transcoders for layers \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(transcoders.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/transcoder/single_layer_transcoder.py:366\u001b[39m, in \u001b[36mload_gemma_scope_transcoder\u001b[39m\u001b[34m(path, layer, device, dtype, revision, **kwargs)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# load the parameters, have to rename the threshold key,\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# as ours is nested inside the activation_function module\u001b[39;00m\n\u001b[32m    365\u001b[39m param_dict = np.load(path_to_params)\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m param_dict = {k: torch.tensor(v, device=device, dtype=dtype) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m param_dict.items()}\n\u001b[32m    367\u001b[39m param_dict[\u001b[33m\"\u001b[39m\u001b[33mactivation_function.threshold\u001b[39m\u001b[33m\"\u001b[39m] = param_dict[\u001b[33m\"\u001b[39m\u001b[33mthreshold\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    368\u001b[39m param_dict[\u001b[33m\"\u001b[39m\u001b[33mW_enc\u001b[39m\u001b[33m\"\u001b[39m] = param_dict[\u001b[33m\"\u001b[39m\u001b[33mW_enc\u001b[39m\u001b[33m\"\u001b[39m].T.contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:899\u001b[39m, in \u001b[36m__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:257\u001b[39m, in \u001b[36mNpzFile.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28mbytes\u001b[39m.seek(\u001b[32m0\u001b[39m)\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m magic == \u001b[38;5;28mformat\u001b[39m.MAGIC_PREFIX:\n\u001b[32m    249\u001b[39m     \u001b[38;5;66;03m# FIXME: This seems like it will copy strings around\u001b[39;00m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m#   more than is strictly necessary.  The zipfile\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;66;03m#   (or at least uncompress) the data\u001b[39;00m\n\u001b[32m    256\u001b[39m     \u001b[38;5;66;03m#   directly into the array memory.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.read_array(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mbytes\u001b[39m,\n\u001b[32m    259\u001b[39m         allow_pickle=\u001b[38;5;28mself\u001b[39m.allow_pickle,\n\u001b[32m    260\u001b[39m         pickle_kwargs=\u001b[38;5;28mself\u001b[39m.pickle_kwargs,\n\u001b[32m    261\u001b[39m         max_header_size=\u001b[38;5;28mself\u001b[39m.max_header_size\n\u001b[32m    262\u001b[39m     )\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/numpy/lib/_format_impl.py:870\u001b[39m, in \u001b[36mread_array\u001b[39m\u001b[34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[39m\n\u001b[32m    868\u001b[39m             read_size = \u001b[38;5;28mint\u001b[39m(read_count * dtype.itemsize)\n\u001b[32m    869\u001b[39m             data = _read_bytes(fp, read_size, \u001b[33m\"\u001b[39m\u001b[33marray data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m             array[i:i + read_count] = numpy.frombuffer(data, dtype=dtype,\n\u001b[32m    871\u001b[39m                                                      count=read_count)\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m array.size != count:\n\u001b[32m    874\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    875\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to read all data for array. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    876\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    877\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcould only read \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    878\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(file seems not fully written?)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    879\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "transcoder_name = \"gemma\" #\"gemma\" mntss/clt-gemma-2-2b-426k\n",
    "model = ReplacementModel.from_pretrained(model_name, transcoder_name, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcZNR0egmS8l"
   },
   "source": [
    "Next, set your attribution arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5XBwNyq4mS8l"
   },
   "outputs": [],
   "source": [
    "prompt = \"Fact: The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    "max_n_logits = 10   # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size=256  # Batch size when attributing\n",
    "offload='disk' if IN_COLAB else 'cpu' # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXfD-5GrmS8l"
   },
   "source": [
    "Then, just run attribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wx2XiXVjmS8l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.51s\n",
      "Found 8296 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.09s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7148\n",
      "Will include 8192 of 8296 feature nodes\n",
      "Input vectors built in 1.81s\n",
      "Phase 3: Computing logit attributions\n",
      "<sys>:0: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.08s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 8192/8192 [00:01<00:00, 4368.04it/s]\n",
      "Feature attributions completed in 1.88s\n",
      "Attribution completed in 9.61s\n"
     ]
    }
   ],
   "source": [
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=model,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUn1YKnUmS8l"
   },
   "source": [
    "We now have a graph object! We can save it as a .pt file, but be warned that it's large (~167MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2tLE4FzdmS8m"
   },
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'austin.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "# graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3cdLLfJmS8m"
   },
   "source": [
    "Given this object, we can create the graph files that we need to visualize the graph. Give it a slug (name), and set the node / edge thresholds for pruning. Pruning removes unimportant nodes and edges from your graph; lower thresholds (i.e., more aggressive pruning) results in smaller graphs. These may be easier to interpret, but explain less of the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vh8HPtimmS8m"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m node_threshold=\u001b[32m0.8\u001b[39m  \u001b[38;5;66;03m# keep only the minimum # of nodes whose cumulative influence is >= 0.8\u001b[39;00m\n\u001b[32m      4\u001b[39m edge_threshold=\u001b[32m0.98\u001b[39m  \u001b[38;5;66;03m# keep only the minimum # of edges whose cumulative influence is >= 0.98\u001b[39;00m\n\u001b[32m      6\u001b[39m create_graph_files(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     graph_or_path=graph_path,  \u001b[38;5;66;03m# the graph to create files for\u001b[39;00m\n\u001b[32m      8\u001b[39m     slug=slug,\n\u001b[32m      9\u001b[39m     output_path=graph_file_dir,\n\u001b[32m     10\u001b[39m     node_threshold=node_threshold,\n\u001b[32m     11\u001b[39m     edge_threshold=edge_threshold\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'graph_path' is not defined"
     ]
    }
   ],
   "source": [
    "slug = \"dallas-austin\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = './graph_files'  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold=0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    "edge_threshold=0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph_path,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQuFE-eimS8m"
   },
   "source": [
    "Now, you can visualize the graph using the following commands! This will spin up a local server to act as the frontend.\n",
    "\n",
    "**If you're running this notebook on a remote server, make sure that you set up port forwarding, so that the chosen port is accessible on your local machine too.**\n",
    "\n",
    "You can select nodes by clicking on them. Ctrl/Cmd+Click on nodes to pin and unpin them to your subgraph. G+Click on nodes in the subgraph to group them together into a supernode; G+Click on the X next to a supernode to dissolve it. Click on the edit button to edit node descriptions, and click on supernode description to edit that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gMZ8Ee-KmS8m"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 98] Address already in use",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcircuit_tracer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfrontend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlocal_server\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serve\n\u001b[32m      3\u001b[39m port = \u001b[32m8046\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m server = serve(data_dir=\u001b[33m'\u001b[39m\u001b[33m./graph_files/\u001b[39m\u001b[33m'\u001b[39m, port=port)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IN_COLAB:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m output \u001b[38;5;28;01mas\u001b[39;00m colab_output  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/circuit_tracer/frontend/local_server.py:216\u001b[39m, in \u001b[36mserve\u001b[39m\u001b[34m(data_dir, frontend_dir, port)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Create a partially applied handler class with configured directories\u001b[39;00m\n\u001b[32m    214\u001b[39m handler = functools.partial(CircuitGraphHandler, frontend_dir=frontend_dir, data_dir=data_dir)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m httpd = ReusableTCPServer((\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, port), handler)\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Start the server in a thread\u001b[39;00m\n\u001b[32m    219\u001b[39m server_thread = threading.Thread(target=httpd.serve_forever, daemon=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/socketserver.py:457\u001b[39m, in \u001b[36mTCPServer.__init__\u001b[39m\u001b[34m(self, server_address, RequestHandlerClass, bind_and_activate)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bind_and_activate:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         \u001b[38;5;28mself\u001b[39m.server_bind()\n\u001b[32m    458\u001b[39m         \u001b[38;5;28mself\u001b[39m.server_activate()\n\u001b[32m    459\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/socketserver.py:478\u001b[39m, in \u001b[36mTCPServer.server_bind\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m.allow_reuse_port \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(socket, \u001b[33m\"\u001b[39m\u001b[33mSO_REUSEPORT\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    475\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.address_family \u001b[38;5;129;01min\u001b[39;00m (socket.AF_INET, socket.AF_INET6)\n\u001b[32m    476\u001b[39m ):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m \u001b[38;5;28mself\u001b[39m.socket.bind(\u001b[38;5;28mself\u001b[39m.server_address)\n\u001b[32m    479\u001b[39m \u001b[38;5;28mself\u001b[39m.server_address = \u001b[38;5;28mself\u001b[39m.socket.getsockname()\n",
      "\u001b[31mOSError\u001b[39m: [Errno 98] Address already in use"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "\n",
    "port = 8046\n",
    "server = serve(data_dir='./graph_files/', port=port)\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import output as colab_output  # noqa\n",
    "    colab_output.serve_kernel_port_as_iframe(port, path='/index.html', height='800px', cache_in_notebook=True)\n",
    "else:\n",
    "    from IPython.display import IFrame\n",
    "    print(f\"Use the IFrame below, or open your graph here: f'http://localhost:{port}/index.html'\")\n",
    "    # display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800px\"\n",
       "            src=\"http://localhost:8046/index.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f648369e210>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(IFrame(src=f'http://localhost:{port}/index.html', width='100%', height='800px'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDGiO8jBmS8m"
   },
   "source": [
    "Once you're done, you can stop the server with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "185O1Ck1mS8m"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'server' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m server.stop()\n",
      "\u001b[31mNameError\u001b[39m: name 'server' is not defined"
     ]
    }
   ],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98579UbGmS8m"
   },
   "source": [
    "Congrats, you're done! Go to `intervention_demo.ipynb` to see how to perform interventions, or check out `gemma_demo.ipynb` and `llama_demo.ipynb` for examples of worked-out test examples. Read on for a bit more info aabout the Graph class and pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkgM1cBCmS8m"
   },
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGnU9l1zmS8m"
   },
   "source": [
    "Earlier, you created a graph object. Its adjacency matrix / edge weights are stored in `graph.adjacency_matrix` in a dense format; rows are target nodes and columns are source nodes. The first `len(graph.real_features)` entries of the matrix represent features; the `i`th entry corresponds to the `i`th feature in `graph.real_features`, given in `(layer, position, feature_idx)` format. The next `graph.cfg.n_layers * graph.n_pos` entries are error_nodes. The next `graph.n_pos` entries are token nodes. The final `len(graph.logit_tokens)` entries are logit nodes.\n",
    "\n",
    "The value of the cell `graph.adjacency_matrix[target, source]` is the direct effect of the source node on the target node. That is, it tells you how much the target node's value would change if the source node were set to 0, while holding the attention patterns, layernorm denominators, and other feature activations constatnt. Thus, if the target node is a feature, this tells you how much the target feature would change; if the target node is a logit, this tells you how much the (de-meaned) value of the logit would change.\n",
    "\n",
    "Note that `gemma-2-2b` is model (family) that uses logit softcapping. This means that a softcap function, `softcap(x) = t * tanh(x/t)` is used to constrain the logits to fall within (-t, t); `gemma-2-2b` uses `t=30`. For such models, we predict the change in logits *pre-softcap*, as the nonlinearity introduced by softcapping would cause our attribution to yield incorrect / approximate direct effect values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWTV8i9zmS8n"
   },
   "source": [
    "### Pruning\n",
    "Given a graph, you might want to prune it, as it will otherwise contain many low-impact nodes and edges that clutter the circuit diagram while adding little information. We enable you to prune nodes by absolute influence, i.e. the total impact that the nodes have on the logits, direct and indirect. The default threshold is 0.8: this means we will keep the minimum number of nodes required to capture 80% of all logit effects. Similarly, the edge_threshold, by default 0.98, means that we will keep the minimum number of edges required to capture 98% of all logit effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from circuit_tracer.subgraph.pruning import trim_graph\n",
    "from circuit_tracer.subgraph.grouping import greedy_grouping\n",
    "from circuit_tracer.subgraph.distance import build_distance_graph_from_clerp\n",
    "\n",
    "graph_file_path = \"graph_files/gemma-clt-fact-dallas-austin_2025-09-25T14-52-21-776Z.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created graph with 26 nodes and 73 edges.\n",
      "Nodes: ['27_22605_10', 'E_26865_9', '20_44686_10', '1_52044_10', 'E_603_10']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding nodes: 100%|██████████| 1/1 [00:00<00:00, 52.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formed 20 clusters.\n",
      "Cluster 0: ('27_22605_10',)\n",
      "Cluster 1: ('E_26865_9',)\n",
      "Cluster 2: ('20_44686_10',)\n",
      "Cluster 3: ('E_603_10',)\n",
      "Cluster 4: ('E_2_0',)\n",
      "Cluster 5: ('E_235292_2',)\n",
      "Cluster 6: ('E_6037_4',)\n",
      "Cluster 7: ('E_2329_7',)\n",
      "Cluster 8: ('E_714_3',)\n",
      "Cluster 9: ('E_10751_8',)\n",
      "Cluster 10: ('0_95057_10',)\n",
      "Cluster 11: ('E_18143_1',)\n",
      "Cluster 12: ('E_573_6',)\n",
      "Cluster 13: ('1_57794_9',)\n",
      "Cluster 14: ('1_12928_10', '1_12928_9')\n",
      "Cluster 15: ('1_52044_10', '1_52044_9')\n",
      "Cluster 16: ('1_72774_9', '1_72774_10')\n",
      "Cluster 17: ('0_91045_10', '0_32742_9')\n",
      "Cluster 18: ('16_89970_9', '7_24743_9')\n",
      "Cluster 19: ('0_40780_10', '0_95475_9')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "G, attr = trim_graph(graph_file_path, crit=\"topk\", top_k = 5)\n",
    "print(f\"Created graph with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "print(\"Nodes:\", list(G.nodes(data=False))[:5])    \n",
    "# distance_graph = np.random.rand(G.number_of_nodes(), G.number_of_nodes())\n",
    "distance_graph = build_distance_graph_from_clerp(G, attr, progress=True, normalize=True)\n",
    "groups, merged_G = greedy_grouping(G, distance_graph, attr, num_groups=20)\n",
    "print(f\"Formed {len(groups)} clusters.\")\n",
    "for i, c in enumerate(groups):\n",
    "    print(f\"Cluster {i}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: Emb: \"  Dallas\", Emb: \"  is\", Emb: \" <bos>\", Emb: \" :\", Emb: \"  capital\", Emb: \"  state\", Emb: \"  The\", Emb: \"  containing\", Emb: \" Fact\", Emb: \"  the\"\n",
      "Layer 1: code snippets, punctuation, is, places, health/cancer, code related, Code/licensing snippets\n",
      "Layer 2: code related,  AssemblyCulture, ble,  AssemblyCulture, Code/licensing snippets\n",
      "Layer 3: Texas, Texas legal matters\n",
      "Layer 4: Texas locations/legal contexts\n",
      "Layer 5: Output \" Austin\" (p=0.439)\n",
      "Saved visualization to subgraphs/subgraph.png\n",
      "Layer 0: Emb: \"  Dallas\", Emb: \"  is\", Emb: \" <bos>\", Emb: \" :\", Emb: \"  capital\", Emb: \"  state\", Emb: \"  The\", Emb: \"  containing\", Emb: \" Fact\", Emb: \"  the\"\n",
      "Layer 1: punctuation, is + places, code snippets + health/cancer\n",
      "Layer 2: ble,  AssemblyCulture +  AssemblyCulture, code related + code related, Code/licensing snippets + Code/licensing snippets\n",
      "Layer 3: Texas + Texas legal matters\n",
      "Layer 4: Texas locations/legal contexts\n",
      "Layer 5: Output \" Austin\" (p=0.439)\n",
      "Saved visualization to subgraphs/merged_subgraph.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('E_26865_9',),\n",
       "  ('E_603_10',),\n",
       "  ('E_2_0',),\n",
       "  ('E_235292_2',),\n",
       "  ('E_6037_4',),\n",
       "  ('E_2329_7',),\n",
       "  ('E_714_3',),\n",
       "  ('E_10751_8',),\n",
       "  ('E_18143_1',),\n",
       "  ('E_573_6',)],\n",
       " [('0_95057_10',), ('0_91045_10', '0_32742_9'), ('0_40780_10', '0_95475_9')],\n",
       " [('1_57794_9',),\n",
       "  ('1_12928_10', '1_12928_9'),\n",
       "  ('1_52044_10', '1_52044_9'),\n",
       "  ('1_72774_9', '1_72774_10')],\n",
       " [('16_89970_9', '7_24743_9')],\n",
       " [('20_44686_10',)],\n",
       " [('27_22605_10',)]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuit_tracer.subgraph.visualization import visualize_clusters\n",
    "\n",
    "visualize_clusters(\n",
    "    G,\n",
    "    draw=True,\n",
    "    filename='subgraphs/subgraph.png',\n",
    "    label_fn=lambda node: attr[node].get('clerp') if attr[node].get('clerp') != \"\" else str(node)\n",
    ")\n",
    "visualize_clusters(\n",
    "    merged_G,\n",
    "    draw=True,\n",
    "    filename='subgraphs/merged_subgraph.png',\n",
    "    label_fn=lambda tuple_node: \" + \".join(attr[node].get('clerp') if attr[node].get('clerp') != \"\" else str(node) for node in tuple_node)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_visualization import visualize_intervention_graph\n",
    "\n",
    "visualize_intervention_graph(merged_G, prompt = prompt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
