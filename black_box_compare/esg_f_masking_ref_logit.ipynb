{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a8c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "# import inspect\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import transformers\n",
    "# import shap\n",
    "torch.manual_seed(0)  # optional: reproducibility\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9fc7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name: str = \"google/gemma-2-2b\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f3e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_token = tokenizer.convert_ids_to_tokens(4)         \n",
    "# mask_token_id = 4   \n",
    "# print(mask_token, mask_token_id)\n",
    "# mask_token_tensor = torch.tensor([[mask_token_id]], device=device)\n",
    "# mask_embedding = model.get_input_embeddings()(mask_token_tensor.clone().contiguous())\n",
    "# print(mask_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56140b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_vocab cell\n",
    "# Uses existing `tokenizer` in the notebook\n",
    "\n",
    "# vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "# print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# # sort by id and print first N entries\n",
    "# sorted_items = sorted(vocab.items(), key=lambda kv: kv[1])\n",
    "# N = 200\n",
    "# print(f\"\\nFirst {N} tokens (id, token):\")\n",
    "# for tok, idx in sorted_items[:N]:\n",
    "#     print(f\"{idx:6d}\\t{tok}\")\n",
    "\n",
    "# write full vocab to a file for inspection (useful since vocab is large)\n",
    "# with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for tok, idx in sorted_items:\n",
    "#         f.write(f\"{idx}\\t{tok}\\n\")\n",
    "# print(\"\\nFull vocab written to 'vocab.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7e2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def egrad_integral_causal_lm(\n",
    "    text: str,\n",
    "    a: float = 0,\n",
    "    b: float = 1.0,\n",
    "    steps: int = 20,\n",
    "    model_name: str = \"google/gemma-2-2b\",\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    mask_token_id: int = 4,  # your mask token ID\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrated gradients for next-token prediction (causal LM).\n",
    "    \n",
    "    Interpolates between a masked embedding baseline and the actual input embeddings,\n",
    "    computing gradients of the target (most probable next token) logit w.r.t. interpolation coefficient.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: \"tokens\", \"acc_attributions\", \"attributions_steps\", \"db_scores\", \"ts\"\n",
    "    \"\"\"\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)           # (1, L)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device) # (1, L)\n",
    "    \n",
    "    # Get base embeddings X: (1, L, d)\n",
    "    embed = model.get_input_embeddings()\n",
    "    with torch.no_grad():\n",
    "        X = embed(input_ids)  # (1, L, d)\n",
    "    L, d = X.shape[1], X.shape[2]\n",
    "    \n",
    "    # Baseline: mask embedding repeated for all positions\n",
    "    mask_token_tensor = torch.tensor([[mask_token_id]], device=device)\n",
    "    mask_embedding = embed(mask_token_tensor.clone().contiguous())  # (1,1,d)\n",
    "    X_RefMask = mask_embedding.repeat(1, L, 1)  # (1, L, d)\n",
    "    \n",
    "    # Run forward pass to find target token (most probable next token)\n",
    "    with torch.no_grad():\n",
    "        out = model(inputs_embeds=X, attention_mask=attention_mask)\n",
    "        logits = out.logits[0, -1, :]  # (vocab_size,) at last position\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        target_id = torch.argmax(probs)\n",
    "        target_prob = probs[target_id].item()\n",
    "        token_for_target = tokenizer.convert_ids_to_tokens([target_id.item()])[0]\n",
    "    \n",
    "    print(f\"Target token: {token_for_target} (id={target_id.item()}, prob={target_prob:.4f})\")\n",
    "    \n",
    "    # Integration grid\n",
    "    t_vals = torch.linspace(a, b, steps, device=device, dtype=X.dtype)\n",
    "    \n",
    "    # Accumulators\n",
    "    attr = np.zeros(L)\n",
    "    attrs = []\n",
    "    db_scores = []\n",
    "    ts = [] \n",
    "    \n",
    "    prev_label_score = None\n",
    "    prev_lg_score = None\n",
    "    sum_dlg = 0.0\n",
    "    \n",
    "    # Padding mask: don't attribute to BOS tokens\n",
    "    padding_mask = torch.ones((L, 1), device=device, dtype=X.dtype)\n",
    "    padding_mask[0] = 0   # skip first token\n",
    "    \n",
    "    for i, t in enumerate(t_vals):\n",
    "        ts.append(t.item())\n",
    "        \n",
    "        # Interpolation coefficient per token: t * ones(L)\n",
    "        ones_L = torch.ones(L, device=device, dtype=X.dtype)\n",
    "        interpolate_v = t * ones_L  # (L,)\n",
    "        interpolated_o = interpolate_v.view(L, 1)  # (L,1)\n",
    "        \n",
    "        # Add a dummy gradient variable\n",
    "        ex = torch.zeros((L, 1), device=device, dtype=X.dtype).requires_grad_(True)\n",
    "        interpolated_o = interpolated_o + ex  # to enable grad\n",
    "        interpolated = interpolated_o.tile((1, d))  # (L, d)\n",
    "        \n",
    "        # Force first token to remain unchanged (coefficient=1)\n",
    "        # interpolated_o[0,0] = 1.0 # hình như phải thêm cái này\n",
    "        interpolated[0, :] = 1.0\n",
    "        \n",
    "        # Interpolated embeddings\n",
    "        X_inter = X * interpolated + X_RefMask * (1 - interpolated)  # (1, L, d)\n",
    "        \n",
    "        # Add epsilon for attribution (only on non-padding positions)\n",
    "        eps = torch.zeros((1, L, 1), device=device, dtype=X.dtype).requires_grad_(True)\n",
    "        X_inter = X_inter + eps * padding_mask.unsqueeze(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(inputs_embeds=X_inter, attention_mask=attention_mask)\n",
    "        logits_step = out.logits[0, -1, :]  # (vocab_size,)\n",
    "        probs_step = F.softmax(logits_step, dim=-1)\n",
    "        \n",
    "        logit_score = logits_step[target_id]\n",
    "        label_score = probs_step[target_id] # gọi tạm là label score nhưng vẫn là prob cho target output logit\n",
    "        \n",
    "        if i == 0:\n",
    "            prev_label_score = label_score\n",
    "            prev_lg_score = logit_score\n",
    "        \n",
    "        dlogit = logit_score - prev_lg_score # diff logit trc softmax\n",
    "        dlb = label_score - prev_label_score # diff prob sau softmax\n",
    "        prev_label_score = label_score\n",
    "        prev_lg_score = logit_score\n",
    "        sum_dlg += dlb.item()\n",
    "        \n",
    "        # Gradient of target logit w.r.t. interpolation coefficient\n",
    "        (grad_eps,) = torch.autograd.grad(logit_score, interpolated_o, retain_graph=False, create_graph=False)\n",
    "        grad_eps_n = grad_eps.squeeze()  # (L,)\n",
    "        \n",
    "        # Normalize gradient\n",
    "        grad_eps_n = grad_eps_n / (torch.sum(grad_eps_n) + 1e-10)\n",
    "        \n",
    "        # Attribution: grad * change in logit \n",
    "        attri = grad_eps_n * dlogit # scale theo dlogit\n",
    "        attri = attri.detach().cpu().numpy()\n",
    "        \n",
    "        attrs.append(attri)\n",
    "        attr += attri\n",
    "        \n",
    "        db_scores.append((\n",
    "            label_score.detach().cpu().item(),\n",
    "            logit_score.detach().cpu().item(),\n",
    "            dlb.detach().cpu().item(),\n",
    "            t.item()\n",
    "        ))\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    print(f\"Sum delta prob: {sum_dlg:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"acc_attributions\": attr,\n",
    "        \"attributions_steps\": attrs,\n",
    "        \"db_scores\": db_scores,\n",
    "        \"ts\": ts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafcee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f6c1f0d88724305865424710da98467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target token: ▁kitten (id=56081, prob=0.8648)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.75 MiB is free. Process 1452554 has 2.71 GiB memory in use. Process 1460377 has 2.53 GiB memory in use. Process 1461816 has 2.59 GiB memory in use. Process 1464024 has 1.59 GiB memory in use. Process 1466204 has 3.82 GiB memory in use. Including non-PyTorch memory, this process has 10.38 GiB memory in use. Of the allocated memory 9.87 GiB is allocated by PyTorch, and 65.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mThe saying goes: Dog is to puppy as cat is to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#text = \"The movie was unhilariously funny, I mean it was bad\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#text = \"The movie was visually stunning, but the plot was predictable\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m#text = \"it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another.\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#text = \"The movie was an emotional masterpiece — the storytelling was powerful, the cinematography was breathtaking, and the music added so much depth to every scene\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#text = \"i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m res = egrad_integral_causal_lm(text, device=device, a=\u001b[32m0\u001b[39m, b=\u001b[32m1\u001b[39m, steps=\u001b[32m20\u001b[39m)\n\u001b[32m     16\u001b[39m tokens = res[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m atts = res[\u001b[33m\"\u001b[39m\u001b[33macc_attributions\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36megrad_integral_causal_lm\u001b[39m\u001b[34m(text, a, b, steps, model_name, device, mask_token_id)\u001b[39m\n\u001b[32m     90\u001b[39m X_inter = X_inter + eps * padding_mask.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m out = model(inputs_embeds=X_inter, attention_mask=attention_mask)\n\u001b[32m     94\u001b[39m logits_step = out.logits[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# (vocab_size,)\u001b[39;00m\n\u001b[32m     95\u001b[39m probs_step = F.softmax(logits_step, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/utils/generic.py:959\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    958\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m959\u001b[39m output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    961\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:548\u001b[39m, in \u001b[36mGemma2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    544\u001b[39m output_hidden_states = (\n\u001b[32m    545\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    546\u001b[39m )\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    549\u001b[39m     input_ids=input_ids,\n\u001b[32m    550\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    551\u001b[39m     position_ids=position_ids,\n\u001b[32m    552\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    553\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    554\u001b[39m     use_cache=use_cache,\n\u001b[32m    555\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    556\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    557\u001b[39m     cache_position=cache_position,\n\u001b[32m    558\u001b[39m     **kwargs,\n\u001b[32m    559\u001b[39m )\n\u001b[32m    561\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/utils/generic.py:1083\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001b[32m   1081\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m-> \u001b[39m\u001b[32m1083\u001b[39m outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1084\u001b[39m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[32m   1085\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:452\u001b[39m, in \u001b[36mGemma2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    450\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    453\u001b[39m     hidden_states,\n\u001b[32m    454\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    455\u001b[39m     attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n\u001b[32m    456\u001b[39m     position_ids=position_ids,\n\u001b[32m    457\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    458\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    459\u001b[39m     use_cache=use_cache,\n\u001b[32m    460\u001b[39m     cache_position=cache_position,\n\u001b[32m    461\u001b[39m     **kwargs,\n\u001b[32m    462\u001b[39m )\n\u001b[32m    464\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:288\u001b[39m, in \u001b[36mGemma2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    286\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.pre_feedforward_layernorm(hidden_states)\n\u001b[32m    287\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_feedforward_layernorm(hidden_states)\n\u001b[32m    289\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    291\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/circuit/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py:62\u001b[39m, in \u001b[36mGemma2RMSNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     59\u001b[39m output = \u001b[38;5;28mself\u001b[39m._norm(x.float())\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Llama does x.to(float16) * w whilst Gemma2 is (x * w).to(float16)\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# See https://github.com/huggingface/transformers/pull/29402\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m output = output * (\u001b[32m1.0\u001b[39m + \u001b[38;5;28mself\u001b[39m.weight.float())\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output.type_as(x)\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 23.64 GiB of which 2.75 MiB is free. Process 1452554 has 2.71 GiB memory in use. Process 1460377 has 2.53 GiB memory in use. Process 1461816 has 2.59 GiB memory in use. Process 1464024 has 1.59 GiB memory in use. Process 1466204 has 3.82 GiB memory in use. Including non-PyTorch memory, this process has 10.38 GiB memory in use. Of the allocated memory 9.87 GiB is allocated by PyTorch, and 65.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# For table step score\n",
    "# LABEL_ID = 0\n",
    "torch.manual_seed(11)\n",
    "random.seed(11)\n",
    "text = \"The saying goes: Dog is to puppy as cat is to\"\n",
    "#text = \"The movie was unhilariously funny, I mean it was bad\"\n",
    "#text = \"The movie was visually stunning, but the plot was predictable\"\n",
    "#text = \"it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another.\"\n",
    "#text = \"it 's also heavy-handed and devotes too much time to bigoted views\"\n",
    "# text = \"while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun\"\n",
    "#text = \"The food tasted awful and the place was dirty\"\n",
    "#text = \"It is summer, but the weather is bad\"\n",
    "#text = \"The movie was an emotional masterpiece — the storytelling was powerful, the cinematography was breathtaking, and the music added so much depth to every scene\"\n",
    "#text = \"i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\"\n",
    "res = egrad_integral_causal_lm(text, device=device, a=0, b=1, steps=20)\n",
    "tokens = res[\"tokens\"]\n",
    "atts = res[\"acc_attributions\"]\n",
    "attr_steps = res[\"attributions_steps\"]\n",
    "db_scores = res[\"db_scores\"]\n",
    "ts = res[\"ts\"]\n",
    "\n",
    "\n",
    "min_idx = int(np.argmin(atts))\n",
    "max_idx = int(np.argmax(atts))\n",
    "db = {  \"min_attr\": float(atts[min_idx]),\n",
    "        \"min_token\": tokens[min_idx],\n",
    "        \"max_attr\": float(atts[max_idx]),\n",
    "        \"max_token\": tokens[max_idx],\n",
    "        \"sum_attr\": float(atts.sum())}\n",
    "print(db)\n",
    "\n",
    "print(f\"Tokens & attributions for target logit:\") \n",
    "for tok in res[\"tokens\"]:\n",
    "    print(f\"{tok:10s}\", end=\" \")\n",
    "for tok in [\"Prob_score\", \"Logit_score\", \"Delta_score\", \"t\"]:\n",
    "    print(f\"{tok:10s}\", end=\" \")   \n",
    "print()\n",
    "for i, attri in enumerate(attr_steps):\n",
    "    for val in attri.tolist():\n",
    "        print(f\"{val:+2.6f}\", end=\"  \")\n",
    "    db_score = db_scores[i]\n",
    "    for v in db_score:\n",
    "        print(f\"{v:+2.6f}\", end=\"  \")\n",
    "    print()\n",
    "for tok, val in zip(res[\"tokens\"], atts.tolist()): \n",
    "    print(f\"{tok:>12s} : {val:+.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
