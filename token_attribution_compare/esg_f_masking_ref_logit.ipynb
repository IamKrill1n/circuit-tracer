{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a8c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "# import inspect\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForQuestionAnswering\n",
    "# import transformers\n",
    "# import shap\n",
    "torch.manual_seed(0)  # optional: reproducibility\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9fc7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name: str = \"google/gemma-2-2b\"\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(device)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f3e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_token = tokenizer.convert_ids_to_tokens(4)         \n",
    "# mask_token_id = 4   \n",
    "# print(mask_token, mask_token_id)\n",
    "# mask_token_tensor = torch.tensor([[mask_token_id]], device=device)\n",
    "# mask_embedding = model.get_input_embeddings()(mask_token_tensor.clone().contiguous())\n",
    "# print(mask_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56140b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_vocab cell\n",
    "# Uses existing `tokenizer` in the notebook\n",
    "\n",
    "# vocab = tokenizer.get_vocab()  # dict: token -> id\n",
    "# print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "# # sort by id and print first N entries\n",
    "# sorted_items = sorted(vocab.items(), key=lambda kv: kv[1])\n",
    "# N = 200\n",
    "# print(f\"\\nFirst {N} tokens (id, token):\")\n",
    "# for tok, idx in sorted_items[:N]:\n",
    "#     print(f\"{idx:6d}\\t{tok}\")\n",
    "\n",
    "# write full vocab to a file for inspection (useful since vocab is large)\n",
    "# with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     for tok, idx in sorted_items:\n",
    "#         f.write(f\"{idx}\\t{tok}\\n\")\n",
    "# print(\"\\nFull vocab written to 'vocab.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f7e2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def egrad_integral_causal_lm(\n",
    "    text: str,\n",
    "    a: float = 0,\n",
    "    b: float = 1.0,\n",
    "    steps: int = 20,\n",
    "    model_name: str = \"google/gemma-2-2b\",\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    mask_token_id: int = 4,  # your mask token ID\n",
    "):\n",
    "    \"\"\"\n",
    "    Integrated gradients for next-token prediction (causal LM).\n",
    "    \n",
    "    Interpolates between a masked embedding baseline and the actual input embeddings,\n",
    "    computing gradients of the target (most probable next token) logit w.r.t. interpolation coefficient.\n",
    "    \n",
    "    Returns:\n",
    "        dict with keys: \"tokens\", \"acc_attributions\", \"attributions_steps\", \"db_scores\", \"ts\"\n",
    "    \"\"\"\n",
    "    # Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize\n",
    "    enc = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = enc[\"input_ids\"].to(device)           # (1, L)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device) # (1, L)\n",
    "    \n",
    "    # Get base embeddings X: (1, L, d)\n",
    "    embed = model.get_input_embeddings()\n",
    "    with torch.no_grad():\n",
    "        X = embed(input_ids)  # (1, L, d)\n",
    "    L, d = X.shape[1], X.shape[2]\n",
    "    \n",
    "    # Baseline: mask embedding repeated for all positions\n",
    "    mask_token_tensor = torch.tensor([[mask_token_id]], device=device)\n",
    "    mask_embedding = embed(mask_token_tensor.clone().contiguous())  # (1,1,d)\n",
    "    X_RefMask = mask_embedding.repeat(1, L, 1)  # (1, L, d)\n",
    "    \n",
    "    # Run forward pass to find target token (most probable next token)\n",
    "    with torch.no_grad():\n",
    "        out = model(inputs_embeds=X, attention_mask=attention_mask)\n",
    "        logits = out.logits[0, -1, :]  # (vocab_size,) at last position\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        target_id = torch.argmax(probs)\n",
    "        target_prob = probs[target_id].item()\n",
    "        token_for_target = tokenizer.convert_ids_to_tokens([target_id.item()])[0]\n",
    "    \n",
    "    print(f\"Target token: {token_for_target} (id={target_id.item()}, prob={target_prob:.4f})\")\n",
    "    \n",
    "    # Integration grid\n",
    "    t_vals = torch.linspace(a, b, steps, device=device, dtype=X.dtype)\n",
    "    \n",
    "    # Accumulators\n",
    "    attr = np.zeros(L)\n",
    "    attrs = []\n",
    "    db_scores = []\n",
    "    ts = [] \n",
    "    \n",
    "    prev_label_score = None\n",
    "    prev_lg_score = None\n",
    "    sum_dlg = 0.0\n",
    "    \n",
    "    # Padding mask: don't attribute to BOS tokens\n",
    "    padding_mask = torch.ones((L, 1), device=device, dtype=X.dtype)\n",
    "    padding_mask[0] = 0   # skip first token\n",
    "    \n",
    "    for i, t in enumerate(t_vals):\n",
    "        ts.append(t.item())\n",
    "        \n",
    "        # Interpolation coefficient per token: t * ones(L)\n",
    "        ones_L = torch.ones(L, device=device, dtype=X.dtype)\n",
    "        interpolate_v = t * ones_L  # (L,)\n",
    "        interpolated_o = interpolate_v.view(L, 1)  # (L,1)\n",
    "        \n",
    "        # Add a dummy gradient variable\n",
    "        ex = torch.zeros((L, 1), device=device, dtype=X.dtype).requires_grad_(True)\n",
    "        interpolated_o = interpolated_o + ex  # to enable grad\n",
    "        interpolated = interpolated_o.tile((1, d))  # (L, d)\n",
    "        \n",
    "        # Force first token to remain unchanged (coefficient=1)\n",
    "        # interpolated_o[0,0] = 1.0 # hình như phải thêm cái này\n",
    "        interpolated[0, :] = 1.0\n",
    "        \n",
    "        # Interpolated embeddings\n",
    "        X_inter = X * interpolated + X_RefMask * (1 - interpolated)  # (1, L, d)\n",
    "        \n",
    "        # Add epsilon for attribution (only on non-padding positions)\n",
    "        eps = torch.zeros((1, L, 1), device=device, dtype=X.dtype).requires_grad_(True)\n",
    "        X_inter = X_inter + eps * padding_mask.unsqueeze(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        out = model(inputs_embeds=X_inter, attention_mask=attention_mask)\n",
    "        logits_step = out.logits[0, -1, :]  # (vocab_size,)\n",
    "        probs_step = F.softmax(logits_step, dim=-1)\n",
    "        \n",
    "        logit_score = logits_step[target_id]\n",
    "        label_score = probs_step[target_id] # gọi tạm là label score nhưng vẫn là prob cho target output logit\n",
    "        \n",
    "        if i == 0:\n",
    "            prev_label_score = label_score\n",
    "            prev_lg_score = logit_score\n",
    "        \n",
    "        dlogit = logit_score - prev_lg_score # diff logit trc softmax\n",
    "        dlb = label_score - prev_label_score # diff prob sau softmax\n",
    "        prev_label_score = label_score\n",
    "        prev_lg_score = logit_score\n",
    "        sum_dlg += dlb.item()\n",
    "        \n",
    "        # Gradient of target logit w.r.t. interpolation coefficient\n",
    "        (grad_eps,) = torch.autograd.grad(logit_score, interpolated_o, retain_graph=False, create_graph=False)\n",
    "        grad_eps_n = grad_eps.squeeze()  # (L,)\n",
    "        \n",
    "        # Normalize gradient\n",
    "        grad_eps_n = grad_eps_n / (torch.sum(grad_eps_n) + 1e-10)\n",
    "        \n",
    "        # Attribution: grad * change in logit \n",
    "        attri = grad_eps_n * dlogit # scale theo dlogit\n",
    "        attri = attri.detach().cpu().numpy()\n",
    "        \n",
    "        attrs.append(attri)\n",
    "        attr += attri\n",
    "        \n",
    "        db_scores.append((\n",
    "            label_score.detach().cpu().item(),\n",
    "            logit_score.detach().cpu().item(),\n",
    "            dlb.detach().cpu().item(),\n",
    "            t.item()\n",
    "        ))\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    print(f\"Sum delta prob: {sum_dlg:.6f}\")\n",
    "    \n",
    "    return {\n",
    "        \"tokens\": tokens,\n",
    "        \"acc_attributions\": attr,\n",
    "        \"attributions_steps\": attrs,\n",
    "        \"db_scores\": db_scores,\n",
    "        \"ts\": ts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eafcee60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86026f7704fb48b09ae8e2f0c66da0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'QuestionAnsweringModelOutput' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      5\u001b[39m text = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33mContext: Architecturally, the school has a Catholic character. Atop the Main Building\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVenite Ad Me Omnes\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mQuestion: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mAnswer:\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m#text = \"The movie was unhilariously funny, I mean it was bad\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#text = \"The movie was visually stunning, but the plot was predictable\"\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#text = \"it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another.\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[38;5;66;03m#text = \"The movie was an emotional masterpiece — the storytelling was powerful, the cinematography was breathtaking, and the music added so much depth to every scene\"\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#text = \"i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m res = egrad_integral_causal_lm(text, device=device, model_name = \u001b[33m\"\u001b[39m\u001b[33mSadat07/bert-SQuAD\u001b[39m\u001b[33m\"\u001b[39m, a=\u001b[32m0\u001b[39m, b=\u001b[32m1\u001b[39m, steps=\u001b[32m101\u001b[39m)\n\u001b[32m     20\u001b[39m tokens = res[\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m atts = res[\u001b[33m\"\u001b[39m\u001b[33macc_attributions\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36megrad_integral_causal_lm\u001b[39m\u001b[34m(text, a, b, steps, model_name, device, mask_token_id)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     43\u001b[39m     out = model(inputs_embeds=X, attention_mask=attention_mask)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     logits = out.logits[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m, :]  \u001b[38;5;66;03m# (vocab_size,) at last position\u001b[39;00m\n\u001b[32m     45\u001b[39m     probs = F.softmax(logits, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m     46\u001b[39m     target_id = torch.argmax(probs)\n",
      "\u001b[31mAttributeError\u001b[39m: 'QuestionAnsweringModelOutput' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "# For table step score\n",
    "# LABEL_ID = 0\n",
    "torch.manual_seed(11)\n",
    "random.seed(11)\n",
    "text = \"\"\"\n",
    "Context: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
    "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
    "Answer:\n",
    "\"\"\"\n",
    "#text = \"The movie was unhilariously funny, I mean it was bad\"\n",
    "#text = \"The movie was visually stunning, but the plot was predictable\"\n",
    "#text = \"it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another.\"\n",
    "#text = \"it 's also heavy-handed and devotes too much time to bigoted views\"\n",
    "# text = \"while the mystery surrounding the nature of the boat 's malediction remains intriguing enough to sustain mild interest , the picture refuses to offer much accompanying sustenance in the way of characterization , humor or plain old popcorn fun\"\n",
    "#text = \"The food tasted awful and the place was dirty\"\n",
    "#text = \"It is summer, but the weather is bad\"\n",
    "#text = \"The movie was an emotional masterpiece — the storytelling was powerful, the cinematography was breathtaking, and the music added so much depth to every scene\"\n",
    "#text = \"i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\"\n",
    "res = egrad_integral_causal_lm(text, device=device, model_name = \"Sadat07/bert-SQuAD\", a=0, b=1, steps=101)\n",
    "tokens = res[\"tokens\"]\n",
    "atts = res[\"acc_attributions\"]\n",
    "attr_steps = res[\"attributions_steps\"]\n",
    "db_scores = res[\"db_scores\"]\n",
    "ts = res[\"ts\"]\n",
    "\n",
    "\n",
    "min_idx = int(np.argmin(atts))\n",
    "max_idx = int(np.argmax(atts))\n",
    "db = {  \"min_attr\": float(atts[min_idx]),\n",
    "        \"min_token\": tokens[min_idx],\n",
    "        \"max_attr\": float(atts[max_idx]),\n",
    "        \"max_token\": tokens[max_idx],\n",
    "        \"sum_attr\": float(atts.sum())}\n",
    "print(db)\n",
    "\n",
    "print(f\"Tokens & attributions for target logit:\") \n",
    "for tok in res[\"tokens\"]:\n",
    "    print(f\"{tok:10s}\", end=\" \")\n",
    "for tok in [\"Prob_score\", \"Logit_score\", \"Delta_score\", \"t\"]:\n",
    "    print(f\"{tok:10s}\", end=\" \")   \n",
    "print()\n",
    "for i, attri in enumerate(attr_steps):\n",
    "    for val in attri.tolist():\n",
    "        print(f\"{val:+2.6f}\", end=\"  \")\n",
    "    db_score = db_scores[i]\n",
    "    for v in db_score:\n",
    "        print(f\"{v:+2.6f}\", end=\"  \")\n",
    "    print()\n",
    "for tok, val in zip(res[\"tokens\"], atts.tolist()): \n",
    "    print(f\"{tok:>12s} : {val:+.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
